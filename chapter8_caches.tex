\documentclass[12pt]{article}
%\documentclass[10pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{authblk}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{cancel}
\usepackage{soul}
\newenvironment{QandA}{\begin{enumerate}[label=\bfseries\arabic*.]\bfseries}
                      {\end{enumerate}}
\newenvironment{answered}{\par\quad\normalfont}{}

\title{CIS501: Caches}
\author[1]{Shreyas S. Shivakumar}

\begin{document}

\maketitle

\section{Lecture Notes}

\begin{QandA}
\item Memory
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item Memory is tricky - it can either be \textit{large} (number of bits that it can store) or it can be \textit{fast} (time taken to access an entry), but it cannot be both.
    \item Volatile Memory:
    \begin{enumerate}
        \item \textbf{Static RAM (SRAM)} is very fast and is most often used in CPU \textit{L1 and L2 caches}. The latency is in the \textit{sub-nanoseconds}.
        \item \textbf{Dynamic RAM (DRAM)} is optimized for density and is slower (in the \textit{tens of nanoseconds}). DRAM is synonymous with regular PC ram and the speeds associated (3200MHz) with RAM is it's refresh rate. \textit{Refreshing} RAM is not required in \textit{Static RAM}.
    \end{enumerate}
    \item Non-Volatile Memory: Magnetic Disks, Flash RAM \textit{etc.}
    \item Good caching strategies are important because if you have a \textit{cache miss}, you will be forced to access lower levels of memory which are very expensive (\textit{slow}) operations.
\end{itemize}
\end{answered}

\item The Memory Wall
\begin{answered}
\textit{Processor speeds are increasing at a faster rate than memory speeds}. 

Clever designs are required to harness the effective speeds of the CPU because a 4GHz processor might be wasted on a system whose memory access latency is 12 milliseconds (\textit{magnetic disk}).
\end{answered}

\item Memory Hierarchy
\begin{answered}
As hinted above, a processor can only compute as fast as it can access memory. For \textit{example:} a 3GHz processor can execute ADD instructions in 0.33ns. However, your average \textit{main memory} latency is more than 33ns. And LDR and STR instructions require even more time.

But it's impossible to have memory that can \textit{keep up} with the fast processor clock rates while having sufficient amount of space for running programs. How do you address this problem? \textit{A \textbf{hierarchy} of memory}.

\textit{Organization:}

The goal is to have multiple layers of memory between the \textit{CPU} and the \textit{Hard Disk} that try to deliver both \textit{fast} memory and \textit{large} memory in a hierarchical manner, with the \textit{fastest} layers of memory, \textbf{L1 cache} (I\$ and D\$), \textbf{L2 cache} and \textbf{L3 cache} being close to the CPU, followed by \textbf{Main Memory} and eventually the \textbf{Disk}.

\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
Level & Memory Type & Managed By \\
\hline
0 & Registers & Compiler \\ 
1 & Primary Cache - I\$ and D\$ (L1) & Hardware \\ 
2 & Second and Third Cache (L2, L3) & Hardware \\ 
3 & Main Memory & OS \\ 
4 & Disk & OS \\ 
\hline
\end{tabular}
\end{center}

\textit{Note:} Level 0, Level 1 and Level 2 are usually \textbf{on-chip}. Main Memory is made of \textit{DRAM} and is usually of in the order of 8GB to 16GB on professional machines. 

\textit{Note:} Chips today (Intel Core i7) are usually 30-70\% cache by area.

\end{answered}

\item Locality
\begin{answered}
A key strategy in improving memory performance is \textit{Locality}. 
\begin{itemize}
    \item \textbf{Spatial Locality:} if a program recently referenced a chunk of data, it is likely to refer to data \textit{near} (in memory) that chunk of data soon. To improve performance, we can \textbf{proactively} fetch large chunks of data including data nearby and keep it available for fast access since the program will be likely to request it soon anyway. \textit{Example:} arrays.
    \item \textbf{Temporal Locality:} if a program recently referenced data, it is likely to refer to that data \textit{again} soon. To improve performance, we can \textbf{reactively} "cache" this data in small, fast memory so that the program can use it again quickly. \textit{Example:} counter variables.
\end{itemize}
\end{answered}

\item Cache Overview
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item A cache is a hardware \textit{hashtable}.
    \item A cache is typically organized as a number of individual cache \textbf{blocks} or cache \textbf{lines}. For \textit{example:} a 4KB cache can be organized as 1024 $\times$ 4B ($4 \times 8$ = 32 bits) blocks / lines. Since there are 1024 blocks, you will need $log_{2}(1024) = 10$ bits to index into this cache. These are \textbf{Index Bits}.
    \item \textbf{Offset Bits:} to index to specific bytes within a cache block, the \textit{Least Significant Bits (LSB)} are used. For the above \textit{example}, since there are 4 bytes in a cache block, you would need $log_{2}(4)=2$ offset bits along with the 10 \textit{Index Bits}.
    
    \textit{Q: Offset Bits aside, Why should we use the remaining 10 bits of Least Significant Bits as Index bits instead of maybe the Most Significant Bits?}
    
    \quad Lower order bits have a higher entropy in the general execution of a program. Higher order bits also are at risk of hashing to the same entry.
    
    \item \textbf{Tag Bits:} How can you know what blocks, if any, are in the cache? In the previous \textit{example}, the remaining bits (32 bits - 10 bits - 2 bits = 20 bits) are used to \textit{tag} each cache word. These tag bits are entered into a separate (and parallel) memory structure called the \textbf{Tag Array}. Additional to the 20 bits is a single \textit{Valid Bit} to indicate if there is information in the cache blocks or not. 
    
    \textit{Q: Since this mechanism adds extra hardware (it is now doubled) to the cache, is it much slower?}
    
    \quad There is a small overhead, but since these two hardware structures can be \textbf{accessed in parallel}, performance is not affected too much.
    
    \item In modern implementations, more than one address is stored in each cache block since typical architectures have more than single byte granularity. Therefore, the entries in each block come from a contiguous set of addresses and inherently exhibit \textit{spatial locality}. 
    
    \item At this basic level, to remember are:
    \begin{itemize}
        \item Index Bits : which row in cache?
        \item Offset Bits : which column (byte) in cache?
        \item Tag Bits : different memory being mapped to the same cache block?
        \item Tag Array : where are tag bits stored?
        \item Valid Bit : is this a valid cache entry?
    \end{itemize}
\end{itemize}
\end{answered}

\item Cache Miss
\begin{answered}
\textit{What if the data that was requested isn't in the cache? Who handles this?}

A \textbf{Cache Controller} is a finite state machine that is responsible for remembering the address of data that resulted in a \textit{cache miss}. It accesses the next level of memory in order to acquire the necessary data. It must wait until it receives a response and then must write the required information and tags into the proper location in the cache. 
\end{answered}

\item Glossary
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item Access : read or write to cache
    \item Hit : required data was found in cache
    \item Miss : required data was not found in cache
    \item Fill : bringing required data into the cache
    \item Miss Rate ($\%_{misses}$) : ratio of misses to accesses
    \item Access Latency ($t_{access}$) : time to check cache
    \item Miss Latency ($t_{miss}$) : time to read data into cache
\end{itemize}
\textit{Note:} A \textbf{Fill} usually also entails the costs involved in \textit{evicting} data that was already in the cache to make room for the new data that is incoming. 
\end{answered}

\item Performance Equation
\begin{answered}
\vspace{-0.45cm}
\begin{equation}
    t_{average} = t_{access} + (\%_{misses} \times t_{miss})
\end{equation}
\textit{Note:} $t_{access}$ is paid regardless of whether it was a hit or a miss. It is the cost associated with reading or writing to the cache.

\textit{Note:} Since a cache miss is technically a function of all the latency associated with retrieving data from the different layers of memory, this above equation is usually \textbf{recursive} up-to the number of layers in the memory hierarchy.

\textit{What's the simplest way to reduce $\%_{misses}$?}

\quad Increase capacity! Miss rates will decreases monotonically but at a certain size of \textbf{working set} you will notice diminishing returns. And with the increased capacity, $t_{access}$ inevitably increases, resulting in lower performance. Intuitively, remembering more $\implies$ miss less, \textbf{but} remembering more $\implies$ access it slower.

\textit{Example \#1:} 

If you have a simple pipeline with $CPI_{base}=1$ and 30\% of instructions are LDR and STR. If the L1 cache has $\%_{miss}=2\%$ for the Instruction Cache (I\$) and $\%_{miss}=10\%$ for the Data Cache (D\$) with a miss penalty of $t_{miss}=10$ cycles for both, what is the new $CPI_{new}$?

\begin{equation*}
    CPI_{new} = CPI_{base} + \%_{I-miss} \times t_{I-miss} + \%_{D-miss} \times t_{D-miss}
\end{equation*}
\begin{equation*}
    CPI_{new} = 1.0 + 0.02 \times 10 + 0.1 \times 10 = 1 + 0.2 + 0.3 = 1.5
\end{equation*}
\end{answered}

\item Cache Organization Example
\begin{answered}
A system has 4-bit addresses ($\rightarrow$ 16B memory) and an 8B cache with a block size of 2B. What are the \textit{Index Bits}, \textit{Offset Bits} and \textit{Tag Bits}? 

The number of rows (sets) is: $\frac{\text{cache capacity}}{\text{block size}}$ = $\frac{8B}{2B} = 4$

\textbf{Offset Bits:} The number of bytes in a block is 2, therefore you will need $log_{2}(2)=1$ offset bits.

\textbf{Index Bits:} There are 4 rows (sets) of cache blocks / lines, therefore you will need $log_{2}(4)=2$ index bits.

\textbf{Tag Bits:} The remaining bits will be used as tag bits, i.e 4 (total) - 1 (offset) - 2 (index) = 1 tag bit. 
\end{answered}

\ 

\item Performance Optimizations
\begin{answered}
As we have noticed, increasing the size of the cache doesn't necessarily result in improvement in performance. We need to think of other mechanisms that can help manipulate the $\%_{misses}$ by changing how the cache is organized, given that it has a fixed capacity.

\begin{enumerate}
    \item Increasing Block Size
    \item Adding Associativity
\end{enumerate}
\end{answered}

\item Increasing Block Size
\begin{answered}
This seems valid and is directly tied to the notion of \textit{spatial locality}. 

\textit{Q: How do the different bits change with increasing block size?}

\quad If the size of the blocks increases, each block will have more bytes, and therefore will require \textbf{more offset bits} to individually index into them. Since the size of the cache is fixed, this will mean that the number of sets / rows will actually reduce, resulting in \textbf{fewer index bits}. The tag bits remain unchanged. 

\textit{Q: Can any of these go to 0 bits?}

\quad This is possible, if the block size is made equal to the size of the entire cache, this would mean that there is a single cache block / line, which will require no index bits. 

\textit{Q: Since the number of tag bits stays the same during the increase in block size, why does the overhead associated (\textbf{tag overhead}) with the tag reduce?}

\quad While the tags remain at the same size, the number of total tags is reduced. Since each tag is associated with a set / row, and the number of sets / rows reduces, you will have fewer tag entries resulting in a lower overhead. 

\begin{equation*}
    \text{Tag Overhead} = \frac{\text{Number of Tag Bits}}{\text{Block Size in Bits}}
\end{equation*}

\textit{Q: What are the implications of larger block sizes on \textbf{performance?}}

\quad Since the block size is larger, more \text{adjacent data} is brought in per operation resulting in more \textbf{spatial pre-fetching} which can reduce the $\%_{misses}$ upto a point. However, after a certain point $\%_{misses}$ starts to increase because (a) potentially \textbf{useless data is also transferred} and (b) we might be \textbf{prematurely replacing} useful data.

\textit{Q: How does increasing block size affect the miss penalty / latency $t_{miss}$?}

\quad Technically, since the blocks are larger, they should take longer to \textbf{read, transfer} and \textbf{fill}. However, if you use \textbf{Critical World First (or) Early Restart} you can avoid this effect on isolated misses. This technique allows for the requested word to be fetched first and the remaining words are filled in the background. This will not, however, improve the situation where a cluster of misses occurs as this will turn into a \textit{bandwidth problem} under those circumstances. 

\end{answered}

\item Associativity

\ 

\begin{answered}
\textbf{Cache Conflicts:} If you had had 16B of Main Memory and an 8B cache, can multiple addresses in memory map to the same location in the cache? \textbf{Yes}. You will essentially have 8 situations of conflict.

For \textit{example:} addresses 0\textbf{01}0 and 1\textbf{01}0 will get mapped to the same location in the cache because they have the same \textbf{index bits}. Even if 0\textbf{01}0 was the only element in the cache, with 7 remaining bytes free, 1\textbf{01}0 would still conflict with it. Can we fix this? \textbf{Yes}, but this will take further re-organization of the cache structure.

\ 

\textit{Introduction:}

In a \textbf{Direct Mapped Cache}, there is only one cache block where a memory block (\textit{example:} 12) can be found, and if there are 8 blocks (and rows), then memory block 12 will get mapped to (12 modulo 8 = 4), i.e block id 4 in the cache. 

In a \textbf{2-Way Set Associative Cache}, there would be four sets (rows) and memory block 12 must be in either one of the two (12 modulo 4 = 0) cache blocks. 

In a \textbf{Fully Associative Cache}, the memory block 12 can appear in any of the eight cache blocks. 

\

\textit{Q: What is the effect of increasing \textbf{associativity} on the different bits?}

\quad If associativity increases, the number of index bits will decrease and the number of tag bits will increase. For \textit{example:}, a direct mapped cache will require more index bits than a 2-way set associative cache. 

\ 

\textit{Q: What is the lookup procedure?}

\quad As before, the \textbf{index bits} are used to find the set / row. The data and tags in all frames are read in parallel. If any of them match and the \textbf{valid bit} is set, then it is a \textit{Hit}. 

\

\newpage 

\textbf{Replacement Policies}

With N-Way Associative Caches, what happens during a \textit{Cache Miss}? Which block in a set should be evicted?
\begin{itemize}
    \item Random
    \item FIFO
    \item \textbf{Least Recently Used (LRU)} : works well in practice. Requires additional \textbf{LRU bits} - for a 2-Way Set Associative cache, this is just a single bit.
    \item Not Most Recently Used (NMRU) : approximation of LRU, easier to implement
    \item Belady's : replace what will be used furthest in the future - impractical but good for simulation and pre-recorded traces 
\end{itemize}

\textit{Q: What is the impact of associativity on \textbf{performance}?}

\quad With higher associative caches, you will have a lower $\%_{misses}$, but at a certain point you will have diminishing returns because $t_{access}$ will increase. 

\textit{Q: Does associativity need to be powers of 2?}

\quad No it does not!
\end{answered}

\item Caches and Stores
\begin{answered}
Everything we have talked about so far has been in the context of reading from the cache, such as during a LOAD instruction. What about \textit{STORE} instructions that write to the cache?

\quad This will mainly involve the Data Cache (D\$) since the Instruction Cache (I\$) is basically read-only. 

\quad Issues arise with \textit{write} operations because as we've hinted at before, there are \textbf{multiple copies} of a particular item across different levels of memory. When a \textit{write/store} operation is performed, how do we \textbf{keep track} of all these different copies and update them accordingly? And how to we guarantee \textbf{consistency} across the different memory levels?

\quad Another problem is with \textbf{tag / data access}: With read operations both tag and data arrays could be accessed in parallel. If there was a \textit{tag mismatch}, that meant that the data was wrong and you could just \textit{stall} until the right data is fetched. What about during a \textbf{write}?

\quad We cannot \textbf{read tag} and \textbf{write data} in parallel because if you happen to write the wrong data - you've made a permanent change and it is hard to recover from this. Therefore, with \textit{write} operations, this is a \textbf{two-step} process where:
\begin{itemize}
    \item You first match the tag
    \item Then write to the matching \textit{way} 
\end{itemize}
This could introduce structural hazards and will require bypassing to avoid unnecessary stalls.
\end{answered}

\item Propagating Writes
\begin{answered}
\textit{Q: Since lower levels of memory (basically the entire memory hierarchy) need to be updated by write instructions, when should you propagate these updated values?}

\quad There are two approaches to this: \textbf{Write-Through} and \textbf{Write-Back}. Modern systems will often use both of these techniques at different levels in memory. 

\quad \textit{Q: Suppose on a STORE instruction, we wrote data into only the data cache (without changing main memory); then, after the write into the cache, memory would have a different value from that in the cache. In such a case, the cache and memory are said to be \textbf{inconsistent}. How can we fix this?}

\quad \textit{Write-Through} : we maintain consistency by always writing the data into both the memory and the cache. \textit{Write-Back} or \textit{Copy-Back} : When a write occurs, the new value is written only to the block in the cache. The modified block is written to the lower level of the hierarchy when it is replaced. 

\textbf{Write-Through:}
\begin{itemize}
    \item This is the \textit{simplest approach}.
    \item This method does require extra bandwidth since you are basically updating the different memory levels each time the cache is updated.
    \item No special hardware is needed.
\end{itemize}

\textbf{Write-Back:}
\begin{itemize}
    \item In this approach, a \textit{write} is propagated only when a block is replaced. This is useful when a processor generates writes faster than the writes can be handled by main memory.
    \item There is a \textbf{dirty bit} associated with each block.
    \item A \textbf{clean block} indicates that all occurrences of this value are consistent, and that there is another copy of this block somewhere. Therefore, evicting a clean block is more straightforward and easy to do.
    \item A \textbf{dirty block} is one that has been updated, meaning that there is a \textit{mismatch} across it's \textit{copies}. These are more challenging to handle.
    \item We use an additional \textit{hardware structure} called the \textbf{Write-Back Buffer (WBB)}. The \textit{intuition} here is to take advantage of caching and only update one level of the hierarchy. Delay the process of updating the rest of the hierarchy for a later time, i.e propagate these changes at a different pace. This does however require an additional amount of bookkeeping to make sure that the changes do actually propagate. 
    \item The contents of the \textit{Write-Back Buffer} will be pushed down the hierarchy and at each stage it will be labeled \textit{dirty} until it reaches the \textit{main memory} and \textit{disk}.
    \item This method uses less bandwidth
    \item \textit{Q: Why use a Write-Back Buffer?} The WBB helps by being an intermediate storage element so that blocks can be \textbf{quickly evicted} and placed here thus freeing up room in the faster memory level. 
\end{itemize}
\end{answered}

\ 

\item Write Misses
\begin{answered}
Next we will see how \textit{write misses} are actually handled and some optimizations that can be made to improve efficiency:

\textit{Q: How are write misses handled?}

\quad \textbf{Write-Allocate:} fills block from next level, then write it. This is commonly used, reduces read misses but requires additional bandwidth. \textbf{Write-Non-Allocate:} just writes to the next level, does not allocate. Results in more read misses but uses less bandwidth. Usually used with \textit{Write-Through} propagation.

\textit{Q: A LOAD instruction during a \textbf{read miss} can't go on without the data so it must stall, but does a STORE instruction during a \textbf{write miss} need to stall?}

\quad No, we can actually keep going. And to do this, we require another hardware structure called the \textbf{Store Buffer}. STORE instructions put addresses / values into the store buffer and keep going. The caveat is that \textbf{LOAD} instructions must now also check the \textit{Store Buffer} in addition to Data Memory. 
\end{answered}

\ 

\item What are the three types of cache misses?
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item \textbf{Compulsory} Miss : It has never seen this address before, and no cache can help with this type of miss, even an infinitely sized cache.
    \item \textbf{Capacity} Miss : The miss occurred because the cache was too small. This miss would occur even if it was a fully associative cache.
    \item \textbf{Conflict} Miss : Miss caused because the associativity was too low. 
    \item \textit{Coherence Miss} : Miss caused by external invalidation.
\end{itemize}
\end{answered}

\item What is a way of reducing Conflict Misses?
\begin{answered}
Conflict Misses occur as a result of insufficient associativity. Adding more associativity to the cache is \textit{expensive} and rarely needed. But what if you could get \textit{associativity on demand} if/when you actually need it? This is what a \textbf{Victim Buffer (VB)} will provide.

\quad The \textbf{Victim Buffer} is a fully associative cache that sits on the L1 cache path. It is \textit{small} and \textit{very fast}. It holds blocks that were evicted from some other level and can provide these at a very low latency when required. If you have a \textit{miss} and find it in the victim buffer, you can quickly place the block back in the L1 cache. It will be faster than accessing the L2 cache. 
\end{answered}

\

\item What is a way of reducing Compulsory Misses?
\begin{answered}
Bring data into the cache \textbf{proactively} / \textbf{speculatively}. Anticipate upcoming miss addresses and bring them into the cache. This is called \textit{Prefetching} and can be done using simple hardware table-driven prefetching. It can identify common strides and access patterns and use predictors to identify and fetch data. 

\textit{Software Prefetching} exists too which is usually inserted by the programmer or the compiler.
\end{answered}

\ 

\item What is a way of reducing Capacity Misses?
\begin{answered}
Capacity misses are usually a result of poor spatial or temporal locality. \textbf{Code Restructuring} is a simple yet effective way to handle this problem in many cases.

\textbf{Loop interchange}: Depending on whether your programming language is \textit{row-major} or \textit{column-major}, a simple interchange of iterator variables can result in a significant increase in performance.

\textbf{Loop blocking}: If accesses are spread out over time, you can exploit temporal locality to fetch and store a cache sized amount of data and more quickly access it. 
\end{answered}

\item Designing a Cache Hierarchy
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item Higher level caches \textit{(L1 cache)} optimize for $t_{access}$. They are more frequently accessed. They have lower capacity and smaller block sizes.
    \item Lower level caches \textit{(L2 and L3 cache)} optimize for $\%_{misses}$. They are less frequently accessed. They have higher capacity, associativity and block size.
\end{itemize}

\textit{Design Choice \#1:} \textbf{Unified} Cache (I\$ and D\$) or \textbf{Split} Cache. A \textit{Split Cache} minimizes structural hazards, but results in smaller, fixed capacities for both data and memory. A \textit{Unified Cache} results in fewer capacity misses since unused instruction capacity can be used for data. 

\textit{Design Choice \#2:} \textbf{Inclusive} Memory or \textbf{Exclusive} Memory. In an \textit{Inclusive} design, a block in L1 is always in L2. In an \textit{Exclusive} design, a block is either in L1 or L2, never in both.
\end{answered}

\ 

\item Hierarchy Performance
\begin{answered}
\begin{equation*}
    t_{avg} = t_{avg-L1}
\end{equation*}
\begin{equation*}
    t_{avg-L1} = t_{access-L1} + (\%_{misses-L1} \times t_{misses-L1})
\end{equation*}
\begin{equation*}
    t_{avg-L1} = t_{access-L1} + (\%_{misses-L1} \times t_{avg-L2})
\end{equation*}
\begin{equation*}
    t_{avg-L1} = t_{access-L1} + (\%_{misses-L1} \times (t_{access-L2} + (\%_{misses-L2} \times t_{misses-L2})))
\end{equation*}

\textit{and so on...}

\ 

\textbf{Note:} When calculating \textbf{miss rate}, there are two ways of performing this calculation (a) misses per instruction or (b) misses per cache access. Depending on the which approach you use, the percentage may vary. 

Misses per instruction is a more straightforward approach and is synonymous to a \textit{global miss rate}. Misses per access is trickier and is a more \textit{local miss rate}. 

\newpage 

\textit{Multi-level Performance Calculation:}

Let's say that a system has 30\% of it's instructions are memory operations. In the L1 cache, the $t_{access}=1$ cycle and $\%_{miss}=5\%$. In the L2 cache, the $t_{access}=10$ cycles and $\%_{miss}=20\%$. In the main memory $t_{access}=50$. What is the CPI of this system?

Percentage of instruction misses in the L1 cache is : 30\% of 5\% $\implies$ 1.5\%.

Percentage of instruction misses in the L2 cache is : 20\% of 1.5\% $\implies$ 0.3\%.

\begin{equation*}
    CPI = 1 + (1.5\% \times 10) + (0.3\% \times 50)
\end{equation*}
\begin{equation*}
    CPI = 1 + 0.15 + 0.15 = 1.3
\end{equation*}
\end{answered}
\end{QandA}

\newpage 

\section{Textbook Notes}

\begin{QandA}

\item Why do we care about good memory architectures?
\begin{answered}
All programs spend much of their time accessing memory and the memory system is necessarily a major factor in determining performance.
\end{answered}

\item What is the general idea behind having a memory hierarchy?
\begin{answered}
The goal is to present the user with as much memory (\textit{i.e size}) as is available in the cheapest technology, while providing access at the \textit{speed} offered by the fastest memory.

\quad Faster memories are more expensive per bit than the slower memories and thus are smaller. The three primary technologies that are present in a hierarchy are \textit{SRAM}, \textit{DRAM} and \textit{Magnetic Disk / Flash}. 
\end{answered}

\item Glossary
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item \textbf{Block / Line:} The \textit{minimum unit of information} that can be either present or not present in a cache.
    \item \textbf{Hit Rate:} The fraction of memory accesses \textit{found} in a level of the memory hierarchy.
    \item \textbf{Miss Rate:} The fraction of memory accesses \textit{not found} in a level of the memory hierarchy.
    \item \textbf{Hit Time:} The time required to access a level of the memory hierarchy, including the time needed to determine whether the access is a hit or a miss.
    \item \textbf{Miss Penalty:} The time required to fetch a block into a level of the memory hierarchy from the lower level, including the time to access the block, transmit it from one level to the other, insert it in the level that experienced the miss, and then pass the block to the requester.
\end{itemize}
\end{answered}

\item Ideal Scenario
\begin{answered}
If the \textit{hit rate} is high enough, the memory hierarchy has an effective access time close to that of the highest (and fastest) level and a size equal to that of the lowest (and largest) level.
\end{answered}

\item What is a \textit{True Hierarchy}?
\begin{answered}
In a true hierarchy, data cannot be present in level \textit{i} unless it is also present in level \textit{i+1}.
\end{answered}

\item What is a \textit{Direct Mapped Cache}?
\begin{answered}
If each word in memory can go to exactly one place in the cache, then it is straightforward to find the word if it is in the cache.
\begin{equation*}
    \text{location} = (\text{block address})\ \%\ (\text{number of blocks in the cache})
\end{equation*}

\textbf{But...}

Because each cache location can contain the contents of a number of different memory locations, (\textit{i.e there is a many-to-one mapping}), how do we know whether the data in the cache corresponds to a requested word? \textbf{Tag bits!}

\textbf{Tag Field:} A field in a table used for a memory hierarchy that contains the address information required to identify whether the associated block in the hierarchy corresponds to a requested word. 

\textbf{Valid Bit:} A field in the tables of a memory hierarchy that indicates that the associate block in the hierarchy contains valid data.

\textit{Q: When would a valid bit be required?}

\quad Think of the situation when a processor first starts up. The cache does not have good data and the tag fields present are actually meaningless. Without the tag bits, we wouldn't be able to tell between random data and actually meaningful information. 
\end{answered}

\item Cache Miss
\begin{answered}
The processing of a cache miss creates a \textit{pipeline stall} as opposed to an interrupt, which would require saving the state of all the registers. 
\end{answered}

\item What are the two directions in which performance can be improved?
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item Reducing the \textit{miss rate} by reducing the probability that two different memory blocks will contend for the same cache location.
    \item Reducing the \textit{miss penalty} by adding additional levels to the hierarchy - multi-level caching. 
\end{itemize}
\end{answered}

\item A \textit{Write Buffer} is used in \textit{Write-Back} to improve performance. When does a \textit{Write Buffer} fail?
\begin{answered}
A \textit{Write Buffer} can stall when the buffer is full and if a write occurs. It is difficult to quantitatively analyze the exact penalty introduced by a write buffer stall. In a simple model, the stalls associated with the write buffer are ignored. 
\end{answered}

\item What is a \textit{Fully Associative Cache}?
\begin{answered}
A cache structure in which a block can be placed in any location in the cache. 

\quad To find a given block in a fully associative cache, \textit{all the entries} in the cache must be searched because a block can be placed in any one. To make the search practical, it is done in parallel with a comparator associated with each cache entry. These comparators significantly \textit{increase the hardware cost}, effectively making fully associative placement practical only for caches with small number of blocks. 

\textit{Note:} In a fully associative cache, there is effectively only one set, and all the blocks must be checked in parallel. Thus, there is no need for an \textit{index field} and the entire address (\textit{excluding offset}) is compared with the \textit{tag entry}. 

\end{answered}

\item What is a \textit{Set Associative Cache}?
\begin{answered}
A cache that has a fixed number of locations (\textit{at least two}) where each block can be placed. 

\quad In a set-associative cache, there are a fixed number of locations where each block can be placed. A set-associative cache with \textit{n} locations for a block is called a \textit{n}-way set associative cache. An \textit{n}-way set-associative cache consists of a number of \textbf{sets}, each of which consists of \textit{n} blocks. Each block in the memory maps to a unique \textbf{set} in the cache. 
\begin{equation*}
    \text{location} = (\text{block address})\ \%\ (\text{number of \textit{sets} in the cache})
\end{equation*}
\vspace{-0.85cm}
\end{answered}

\item What happens when \textit{associativity} is increased?
\begin{answered}
This usually results in a decrease in miss rate. However, there is a potential increase in access time / hit time, so there is a \textit{tradeoff} to be made. 
\end{answered}

\item Where can blocks be placed?
\begin{answered}
\vspace{-0.85cm}
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 Scheme & Number of \textit{sets} & Blocks per \textit{set}\\ [0.5ex] 
 \hline\hline
 Direct Mapped & Number of blocks in cache & 1\\ 
 \hline
 Set Associative & $\frac{Number\ of\ blocks\ in\ cache}{Associativity}$ & Associativity\\
 \hline
 Fully Associative & 1 & Number of blocks in cache\\ [1ex] 
 \hline
\end{tabular}
\end{center}
\end{answered}

\newpage 

\item How is a block found?
\begin{answered}
\vspace{-0.85cm}
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 Associativity & Location method & Comparisons required\\ [0.5ex] 
 \hline\hline
 Direct Mapped & Index bits & 1\\ 
 \hline
 Set Associative & Index set, \textit{search elements} & Associativity\\
 \hline
 Fully Associative & Search cache entries & Size of the cache\\ [1ex] 
 \hline
\end{tabular}
\end{center}
\end{answered}

\item What block is replaced during a \textit{miss}?
\begin{answered}
Typically, either the \textit{Least Recently Used (LRU)} or \textit{Not Most Recently Used (NMRU)}, i.e an approximation of LRU or just Random selection.
\end{answered}

\item How are writes handled?
\begin{answered}
Each level in the hierarchy can use either write-through or write-back. 
\end{answered}

\end{QandA}

\end{document}
