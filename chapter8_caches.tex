\documentclass[12pt]{article}
%\documentclass[10pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{authblk}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{cancel}
\usepackage{soul}
\newenvironment{QandA}{\begin{enumerate}[label=\bfseries\arabic*.]\bfseries}
                      {\end{enumerate}}
\newenvironment{answered}{\par\quad\normalfont}{}

\title{CIS501: Caches}
\author[1]{Shreyas S. Shivakumar}

\begin{document}

\maketitle

\section{Lecture Notes}

\begin{QandA}
\item Memory
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item Memory is tricky - it can either be \textit{large} (number of bits that it can store) or it can be \textit{fast} (time taken to access an entry), but it cannot be both.
    \item Volatile Memory:
    \begin{enumerate}
        \item \textbf{Static RAM (SRAM)} is very fast and is most often used in CPU \textit{L1 and L2 caches}. The latency is in the \textit{sub-nanoseconds}.
        \item \textbf{Dynamic RAM (DRAM)} is optimized for density and is slower (in the \textit{tens of nanoseconds}). DRAM is synonymous with regular PC ram and the speeds associated (3200MHz) with RAM is it's refresh rate. \textit{Refreshing} RAM is not required in \textit{Static RAM}.
    \end{enumerate}
    \item Non-Volatile Memory: Magnetic Disks, Flash RAM \textit{etc.}
    \item Good caching strategies are important because if you have a \textit{cache miss}, you will be forced to access lower levels of memory which are very expensive (\textit{slow}) operations.
\end{itemize}
\end{answered}

\item The Memory Wall
\begin{answered}
\textit{Processor speeds are increasing at a faster rate than memory speeds}. 

Clever designs are required to harness the effective speeds of the CPU because a 4GHz processor might be wasted on a system whose memory access latency is 12 milliseconds (\textit{magnetic disk}).
\end{answered}

\item Memory Hierarchy
\begin{answered}
As hinted above, a processor can only compute as fast as it can access memory. For \textit{example:} a 3GHz processor can execute ADD instructions in 0.33ns. However, your average \textit{main memory} latency is more than 33ns. And LDR and STR instructions require even more time.

But it's impossible to have memory that can \textit{keep up} with the fast processor clock rates while having sufficient amount of space for running programs. How do you address this problem? \textit{A \textbf{hierarchy} of memory}.

\textit{Organization:}

The goal is to have multiple layers of memory between the \textit{CPU} and the \textit{Hard Disk} that try to deliver both \textit{fast} memory and \textit{large} memory in a hierarchical manner, with the \textit{fastest} layers of memory, \textbf{L1 cache} (I\$ and D\$), \textbf{L2 cache} and \textbf{L3 cache} being close to the CPU, followed by \textbf{Main Memory} and eventually the \textbf{Disk}.

\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
Level & Memory Type & Managed By \\
\hline
0 & Registers & Compiler \\ 
1 & Primary Cache - I\$ and D\$ (L1) & Hardware \\ 
2 & Second and Third Cache (L2, L3) & Hardware \\ 
3 & Main Memory & OS \\ 
4 & Disk & OS \\ 
\hline
\end{tabular}
\end{center}

\textit{Note:} Level 0, Level 1 and Level 2 are usually \textbf{on-chip}. Main Memory is made of \textit{DRAM} and is usually of in the order of 8GB to 16GB on professional machines. 

\textit{Note:} Chips today (Intel Core i7) are usually 30-70\% cache by area.

\end{answered}

\item Locality
\begin{answered}
A key strategy in improving memory performance is \textit{Locality}. 
\begin{itemize}
    \item \textbf{Spatial Locality:} if a program recently referenced a chunk of data, it is likely to refer to data \textit{near} (in memory) that chunk of data soon. To improve performance, we can \textbf{proactively} fetch large chunks of data including data nearby and keep it available for fast access since the program will be likely to request it soon anyway. \textit{Example:} arrays.
    \item \textbf{Temporal Locality:} if a program recently referenced data, it is likely to refer to that data \textit{again} soon. To improve performance, we can \textbf{reactively} "cache" this data in small, fast memory so that the program can use it again quickly. \textit{Example:} counter variables.
\end{itemize}
\end{answered}

\item Cache Overview
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item A cache is a hardware \textit{hashtable}.
    \item A cache is typically organized as a number of individual cache \textbf{blocks} or cache \textbf{lines}. For \textit{example:} a 4KB cache can be organized as 1024 $\times$ 4B ($4 \times 8$ = 32 bits) blocks / lines. Since there are 1024 blocks, you will need $log_{2}(1024) = 10$ bits to index into this cache. These are \textbf{Index Bits}.
    \item \textbf{Offset Bits:} to index to specific bytes within a cache block, the \textit{Least Significant Bits (LSB)} are used. For the above \textit{example}, since there are 4 bytes in a cache block, you would need $log_{2}(4)=2$ offset bits along with the 10 \textit{Index Bits}.
    
    \textit{Q: Offset Bits aside, Why should we use the remaining 10 bits of Least Significant Bits as Index bits instead of maybe the Most Significant Bits?}
    
    \quad Lower order bits have a higher entropy in the general execution of a program. Higher order bits also are at risk of hashing to the same entry.
    
    \item \textbf{Tag Bits:} How can you know what blocks, if any, are in the cache? In the previous \textit{example}, the remaining bits (32 bits - 10 bits - 2 bits = 20 bits) are used to \textit{tag} each cache word. These tag bits are entered into a separate (and parallel) memory structure called the \textbf{Tag Array}. Additional to the 20 bits is a single \textit{Valid Bit} to indicate if there is information in the cache blocks or not. 
    
    \textit{Q: Since this mechanism adds extra hardware (it is now doubled) to the cache, is it much slower?}
    
    \quad There is a small overhead, but since these two hardware structures can be \textbf{accessed in parallel}, performance is not affected too much.
    
    \item In modern implementations, more than one address is stored in each cache block since typical architectures have more than single byte granularity. Therefore, the entries in each block come from a contiguous set of addresses and inherently exhibit \textit{spatial locality}. 
    
    \item At this basic level, to remember are:
    \begin{itemize}
        \item Index Bits : which row in cache?
        \item Offset Bits : which column (byte) in cache?
        \item Tag Bits : different memory being mapped to the same cache block?
        \item Tag Array : where are tag bits stored?
        \item Valid Bit : is this a valid cache entry?
    \end{itemize}
\end{itemize}
\end{answered}

\item Cache Miss
\begin{answered}
\textit{What if the data that was requested isn't in the cache? Who handles this?}

A \textbf{Cache Controller} is a finite state machine that is responsible for remembering the address of data that resulted in a \textit{cache miss}. It accesses the next level of memory in order to acquire the necessary data. It must wait until it receives a response and then must write the required information and tags into the proper location in the cache. 
\end{answered}

\item Glossary
\begin{answered}
\vspace{-0.85cm}
\begin{itemize}
    \item Access : read or write to cache
    \item Hit : required data was found in cache
    \item Miss : required data was not found in cache
    \item Fill : bringing required data into the cache
    \item Miss Rate ($\%_{misses}$) : ratio of misses to accesses
    \item Access Latency ($t_{access}$) : time to check cache
    \item Miss Latency ($t_{miss}$) : time to read data into cache
\end{itemize}
\textit{Note:} A \textbf{Fill} usually also entails the costs involved in \textit{evicting} data that was already in the cache to make room for the new data that is incoming. 
\end{answered}

\item Performance Equation
\begin{answered}
\vspace{-0.45cm}
\begin{equation}
    t_{average} = t_{access} + (\%_{misses} \times t_{miss})
\end{equation}
\textit{Note:} $t_{access}$ is paid regardless of whether it was a hit or a miss. It is the cost associated with reading or writing to the cache.

\textit{Note:} Since a cache miss is technically a function of all the latency associated with retrieving data from the different layers of memory, this above equation is usually \textbf{recursive} up-to the number of layers in the memory hierarchy.

\textit{What's the simplest way to reduce $\%_{misses}$?}

\quad Increase capacity! Miss rates will decreases monotonically but at a certain size of \textbf{working set} you will notice diminishing returns. And with the increased capacity, $t_{access}$ inevitably increases, resulting in lower performance. Intuitively, remembering more $\implies$ miss less, \textbf{but} remembering more $\implies$ access it slower.

\textit{Example \#1:} 

If you have a simple pipeline with $CPI_{base}=1$ and 30\% of instructions are LDR and STR. If the L1 cache has $\%_{miss}=2\%$ for the Instruction Cache (I\$) and $\%_{miss}=10\%$ for the Data Cache (D\$) with a miss penalty of $t_{miss}=10$ cycles for both, what is the new $CPI_{new}$?

\begin{equation*}
    CPI_{new} = CPI_{base} + \%_{I-miss} \times t_{I-miss} + \%_{D-miss} \times t_{D-miss}
\end{equation*}
\begin{equation*}
    CPI_{new} = 1.0 + 0.02 \times 10 + 0.1 \times 10 = 1 + 0.2 + 0.3 = 1.5
\end{equation*}
\end{answered}

\item Cache Organization Example
\begin{answered}
A system has 4-bit addresses ($\rightarrow$ 16B memory) and an 8B cache with a block size of 2B. What are the \textit{Index Bits}, \textit{Offset Bits} and \textit{Tag Bits}? 

The number of rows (sets) is: $\frac{\text{cache capacity}}{\text{block size}}$ = $\frac{8B}{2B} = 4$

\textbf{Offset Bits:} The number of bytes in a block is 2, therefore you will need $log_{2}(2)=1$ offset bits.

\textbf{Index Bits:} There are 4 rows (sets) of cache blocks / lines, therefore you will need $log_{2}(4)=2$ index bits.

\textbf{Tag Bits:} The remaining bits will be used as tag bits, i.e 4 (total) - 1 (offset) - 2 (index) = 1 tag bit. 
\end{answered}

\item Performance Optimizations
\begin{answered}
As we have noticed, increasing the size of the cache doesn't necessarily result in improvement in performance. We need to think of other mechanisms that can help manipulate the $\%_{misses}$ by changing how the cache is organized, given that it has a fixed capacity.

\begin{enumerate}
    \item Increasing Block Size
    \item Adding Associativity
\end{enumerate}
\end{answered}

\item Increasing Block Size
\begin{answered}
This seems valid and is directly tied to the notion of \textit{spatial locality}. 

\textit{Q: How do the different bits change with increasing block size?}

\quad If the size of the blocks increases, each block will have more bytes, and therefore will require \textbf{more offset bits} to individually index into them. Since the size of the cache is fixed, this will mean that the number of sets / rows will actually reduce, resulting in \textbf{fewer index bits}. The tag bits remain unchanged. 

\textit{Q: Can any of these go to 0 bits?}

\quad This is possible, if the block size is made equal to the size of the entire cache, this would mean that there is a single cache block / line, which will require no index bits. 

\textit{Q: Since the number of tag bits stays the same during the increase in block size, why does the overhead associated (\textbf{tag overhead}) with the tag reduce?}

\quad While the tags remain at the same size, the number of total tags is reduced. Since each tag is associated with a set / row, and the number of sets / rows reduces, you will have fewer tag entries resulting in a lower overhead. 

\begin{equation*}
    \text{Tag Overhead} = \frac{\text{Number of Tag Bits}}{\text{Block Size in Bits}}
\end{equation*}

\textit{Q: What are the implications of larger block sizes on \textbf{performance?}}

\quad Since the block size is larger, more \text{adjacent data} is brought in per operation resulting in more \textbf{spatial pre-fetching} which can reduce the $\%_{misses}$ upto a point. However, after a certain point $\%_{misses}$ starts to increase because (a) potentially \textbf{useless data is also transferred} and (b) we might be \textbf{prematurely replacing} useful data.

\textit{Q: How does increasing block size affect the miss penalty / latency $t_{miss}$?}

\quad Technically, since the blocks are larger, they should take longer to \textbf{read, transfer} and \textbf{fill}. However, if you use \textbf{Critical World First (or) Early Restart} you can avoid this effect on isolated misses. This technique allows for the requested word to be fetched first and the remaining words are filled in the background. This will not, however, improve the situation where a cluster of misses occurs as this will turn into a \textit{bandwidth problem} under those circumstances. 

\end{answered}

\end{QandA}

\end{document}
