\documentclass[12pt]{article}
%\documentclass[10pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{authblk}
\usepackage{indentfirst}
\usepackage{enumitem}
\newenvironment{QandA}{\begin{enumerate}[label=\bfseries\arabic*.]\bfseries}
                      {\end{enumerate}}
\newenvironment{answered}{\par\quad\normalfont}{}

\title{CIS501: Pipelining}
\author[1]{Shreyas S. Shivakumar}

\begin{document}

\maketitle

\section{Lecture Notes}

\begin{QandA}
   \item What are we trying to improve by introducing this optimization (pipelining)?
        \begin{answered}
        Throughput. In the \textit{laundry example}, pipelining will only enhance performance if you have more than one load of clothes to clean.
        \end{answered}
        
    \item If you have 3 stages - washer, dryer and "folding robot"? How long does one load of laundry take? How long does two loads of laundry take? How long would 100 loads take?
        \begin{answered}
            $1$ load of laundry will take $3$ units of time. $2$ loads of laundry will take $4$ units of time. $100$ loads of laundry will take $104$ units of time. 
            
            To understand why $100$ loads of laundry will take $104$ units of time, understand why $3$ loads of laundry will take $5$ loads of time. \textit{Intuition:} The case(s) when the number of loads of laundry is $\geq$ the number of pipeline stages.
        \end{answered}
        
    \item What is the processor performance equation?
    \begin{answered}
    \begin{equation*}
        Execution\ Time = \frac{Seconds}{Program} = \frac{I \times S \times C}{P \times C \times I}
    \end{equation*}
    \begin{equation*}
        Execution\ Time = \frac{Instructions}{Program} \times \frac{Seconds}{Cycle} \times \frac{Cycles}{Instruction}
    \end{equation*}
    
    For \textit{performance optimization}, we want to ideally minimize all three of the above terms, but sometimes they will pull against each other.
    
    Here $\frac{Instruction}{Program}$ refers to the \textbf{Dynamic Instruction Count}. It is the instructions that the processor will actually run during the course of a program and not just the static instructions (eg: loops).
    \end{answered}
    
    \item Briefly discuss the performance of a Single-Cycle Datapath and mention it's limitations?
    \begin{answered}
    Everything is done within a single clock cycle. This means that the clock cycle is chosen to be the worst-case delay through the circuit - \textit{critical path}. Therefore, performance is limited by the slowest instruction always.
    \end{answered}
        
    \item What are the different stages in the pipelined LC4 datapath?
    \begin{answered}
    There are five different stages in the LC4 datapath, called the \textbf{pipeline depth}, with one instruction in each stage in each cycle:
    \begin{itemize}
        \item Fetch (\textbf{F}) : 
        \item Decode (\textbf{D})
        \item Execute (\textbf{X})
        \item Memory (\textbf{M})
        \item Writeback (\textbf{W})
    \end{itemize}
    \end{answered}
    
    \item How is the clock period selected for this pipelined datapath?
    \begin{answered}
    \begin{equation*}
        Clock\ Period\ =\ MAX(T_{F}, T_{D}, T_{X}, T_{M}, T_{W})
    \end{equation*}    
    \end{answered}
    
    \item What is the CPI of the pipelined architecutre?
    \begin{answered}
    The base CPI is $1$ because instructions enter and leave every cycle, but the actual CPI $\geq$ 1 since the pipeline might \textit{stall}. The individual instruction \textbf{latency} will actually \textbf{increase} due to the overhead of the pipelining process, but the benefits of additional throughput make it worth sacrificing latency for.
    \end{answered}
    
    \item How are the different stages arranged?
    \begin{answered}
    The different stages are delimited by the \textit{pipeline registers}, which are named by the stages they begin at.
    \begin{itemize}
        \item PC Register \textbf{(PC)} \textit{(Fetch)}
        \item Decode Register \textbf{(D)}
        \item Execute Register \textbf{(X)}
        \item Memory Register \textbf{(M)}
        \item Writeback Register \textbf{(W)}
    \end{itemize}
    \end{answered}
    
    \item Pipeline Example:
    \begin{answered}
    Let's look at the following example of three instructions:
    \begin{enumerate}
        \item ADD R1, R2, R3 : \textit{where R3 is the destination register}
        \item LOAD 8(R5), R4 : \textit{where R4 is the destination register}
        \item STORE R6, 4(R7) : \textit{where R7+4 is the destination memory address}
    \end{enumerate}
    
    \textbf{Cycle 1:} 
    
    In the first cycle, we do not yet know that the instruction entering the pipeline is an ADD instruction. This instruction enters the \textit{Fetch Stage} and we identify the address of the next instruction (in \textit{instruction memory}) using the \textbf{Program Counter (PC)}. We then go get those bits from the instruction memory.
    
    \textbf{Cycle 2:} 
    
    In the second cycle, the ADD instruction enters the \textit{Decode Stage} and proceeds to decode the instruction and read from the \textit{Register File} to get the input values (R1 and R2). The decoding process parses the input instruction to establish all the control signals required for this instruction in the proceeding stages.
    
    \quad In this same cycle, the next instruction (LOAD) is in the \textit{Fetch} stage, doing what the ADD instruction did in the previous cycle.
    
    \textbf{Cycle 3:} 
    
    In the third cycle, the ADD instruction enters the \textit{Execute Stage} and proceeds to use the \textbf{ALU} to execute an ADD operation on the two operands. 
    
    \quad In this same cycle, the LOAD instruction enters the \textit{Decode Stage} and the next instruction (STORE) enters the \textit{Fetch Stage}.
    
    \textbf{Cycle 4:}
    
    In the fourth cycle, the ADD instruction goes through the \textit{Memory Stage} even though there's nothing for it to do there. It needs to write the resulting value to register R3, which it will do in the next step. Can be thought of as a \textbf{NO-OP} stage for the ADD instruction.
    
    \quad The other two instructions proceed down the pipeline to \textit{Execute Stage} and \textit{Decode Stage} respectively.
    
    \textbf{Cycle 5:}
    
    In the fifth cycle, the ADD instruction enters the \textit{Writeback Stage}, and then performs a write-back of the ALU result to the \textit{Register File}, selecting the correct destination register.
    
    \quad The other two instructions proceed down the pipeline to \textit{Memory Stage} and \textit{Execute Stage} respectively. 
    
    \textbf{Cycle 6 \& Cycle 7:}
    
    LOAD and STORE pass through the last two stages of the pipeline over the these two cycles.
    
    \end{answered}
    
    \item Pipeline Performance:
    \begin{answered}
    In a single-cycle design, if the clock period is \textbf{50ns} and CPI = 1, then your effective performance is \textbf{50ns/insn}.
    
    In the 5-stage pipeline above, you could naively say the performance is now $\frac{50ns}{5}$ = \textbf{10ns}, but this is not the case for the following reasons:
    \begin{itemize}
        \item Not all stages are uniform in the amount of time they will take.
        \item Pipeline registers add delay to the system
        \item There are more datapaths in pipelined sytems (bypasses)
    \end{itemize}
    
    Therefore longer (deeper) pipelines show diminishing clock frequency gains. However, in our example, we can assume that our clock period = \textbf{10ns} + overheads = \textbf{12ns}. And let's say that our CPI = 1 + pipeline penalty = \textbf{1.5}. Our effective performance will then be \textbf{12ns $\times$ 1.5 = 18ns/insn}.
    \end{answered}
    
    \item Dependencies and Hazards
    \begin{answered}
    A key challenge in pipelining is the identification and handling of dependencies across multiple different instructions.
    
    \textbf{Dependence:} It is a relationship between any two instructions.
    
    \begin{itemize}
        \item Data Dependence: If two instructions use the same storage locations (i.e either the same memory address or register)
        \item Control Dependence: One instruction affects whether another instruction executes at all (i.e such as in a branch instruction)
    \end{itemize}
    
    \textbf{Hazard:} A hazard is a result of the existence of a dependence leading to the possibility of wrong instruction order. The effects of a hazard should not be externally visible. \textbf{Stalls} are used to maintain this order by holding a younger instruction in the same stage and propagating an empty instruction (\textbf{no-op}) instead. Hazards are undesirable because the stalls that are introduce result in reduction in performance.
    \end{answered}
    
    \item Data Hazards
    \begin{answered}
    Ignoring \textit{control hazards} for now, let us look at Data Hazards, i.e when two instructions use the same storage locations (memory or register). 
    
    \textit{Example 1:}
    \begin{enumerate}
        \item ADD R1, R2 $\rightarrow$ \textbf{R3}
        \item ADD \textbf{R3}, R5 $\rightarrow$ R6
    \end{enumerate}
    
    \textit{Example 2:}
    \begin{enumerate}
        \item ADD R1, R2 $\rightarrow$ \textbf{R3}
        \item LOAD 8(\textbf{R3}) $\rightarrow$ R4
        \item ADDI 1, \textbf{R3} $\rightarrow$ R6
        \item STORE 8, R7 $\rightarrow$ \textbf{R3}
    \end{enumerate}
    
    At \textbf{Cycle 4} of this example, ADD is writing it's result to R3. LOAD has already read R3 two cycles ago, which means that the value that it currently has is outdated! ADDI also read R3 one cycle ago which means that it's value is outdated too. STORE is currently in the process of reading R3 but it is unclear whether it has the updated value of R3 or not depending on how the \textit{Register File} was implemented (i.e Read-after-Write or Write-after-Read).
    
    \end{answered}

    \item What are some initial ways we can try fix this problem?
    \begin{answered}
    We can fix this problem in two ways:
    \begin{enumerate}
        \item \textbf{Software Interlocks:} 
        
        The compiler will identify and put two independent instructions between the two instructions that have a dependence. If no independent instructions exist to re-order between them, a \textit{no-op} is placed.
        
        \begin{itemize}
            \item For software interlocks, the CPI is still technically 1 but there are now more instructions. For \textit{example}: 20\% of instructions require the insertion of 1 \textbf{no-op}. 5\% of instructions require the insertion of 2 \textbf{no-ops}.
            \item The total number of instructions then goes up to $1 + 0.20 \times 1 + 0.05 \times 2$ = 1.3 resulting in a 30\% slowdown.
            \item Another problem with software interlocks is the dependence on specific architecture design. These optimizations are pipeline dependent and make it difficult to ensure backward compatibility (e.g if you were to move towards a 7-stage pipeline, this would not work anymore).
        \end{itemize}
        
        \item \textbf{Hardware Interlocks:}
        
        Here, the processor detects the data hazards and fixes them. We are not concerned anymore with compatibility issues.
        
        We can \textit{detect data hazards} by comparing the input register names of the instruction in the \textbf{Decode Stage} with output register names of instructions in the \textbf{Execute Stage} and the \textbf{Memory Stage}.
        
        \begin{equation*}
        \begin{split}
        stall = &\ (Decode.IR.src1 == Execute.IR.dest)\ ||\\ &\ (Decode.IR.src2 == Execute.IR.dest)\ ||\\ &\ (Decode.IR.src1 == Memory.IR.dest)\ ||\\ &\ (Decode.IR.src2 == Memory.IR.dest)
        \end{split}
        \end{equation*}
        
        If any of these conditions hold \textit{True}, then insert a \textbf{no-op} into the pipeline at the next stage. This maneuver is called a \textbf{stall} or a \textbf{bubble}. 
        
        For hardware interlocks, the CPI can be calculated as above. Here 20\% of instructions require 1 cycle stall and 5\% of instructions require a 2\% cycle stall. The effective CPI is still the same as with \textit{software interlocks}, except here the instructions stay at 1 and the CPI increases by 30\%.

    \end{enumerate}
    \end{answered}
    
    \item Bypassing
    \begin{answered}
    For \textit{example}: say we had an instruction [ADD R1, R2 $\rightarrow$ R3] in the \textit{Memory Stage} and an instruction [LOAD 8(R3) $\rightarrow$ R4] in the \textit{Execute Stage}. Since the LOAD instruction is already in the Execute Stage, this tells us that the current value of R3 that the LOAD instruction has is outdated, since the ADD instruction is just about to update it's value. However, at this very moment, nothing catastrophic has happened yet, even though we know it will in the next cycle if left unchecked. 
    
    \textbf{1. MX Bypass:}
    
    \textit{What if we were to pass the updated value of R3 from the Memory Stage back to the Execute Stage, so that the LOAD instruction can now possess the latest value for R3?}
    
    This is called \textbf{Bypassing} or \textbf{Forwarding} and this specific case of forwarding information from the \textit{Memory Stage} to the \textit{Execute Stage} is called \textbf{MX Bypassing}.
    
    There is indeed an additional hardware overhead of performing a bypass operation since we need extra wires and a multiplexer. But these overheads are negligible in terms of the performance gained. Instead of filling that cycle with a \textit{no-op} we were able to make progress at the cost of minimum hardware latency. 
    
    \textbf{2. WX Bypass:}
    
    Similarly, if such a dependence exists across the \textit{Write-back Stage} and the \textit{Execute Stage}, we can perform the same operation and forward this value to the \textit{Execute Stage} with an additional wire and Mux. 
    
    \begin{itemize}
        \item \textit{Which ALU input in the Execute Stage can you bypass to?} You can add similar wires and multiplexers and make bypass operations available to both inputs of the ALU. Both \textbf{ALUinA} and \textbf{ALUinB} bypassing is possible. 
    \end{itemize}
    
    \textbf{3. WM Bypass:}
    
    Another bypassing technique, although helpful in limited circumstances, is WM Bypass. For \textit{example} if you have instructions [LOAD 8(R2) $\rightarrow$ R3] and [STORE R3 $\rightarrow$ 4(R4)]. When the LOAD instruction is in the \textit{Write-back Stage} it writes the updated value of register R3 back to the Register File. However, the STORE instruction needs to write the value from register R3 to memory address $4(R4)$. Currently, the STORE instruction contains an outdated value of R3 and hence a bypass from the \textit{Write-back Stage} to the \textit{Memory Stage} makes sense.
    
    However, let us look at an \textbf{important contradicting example}:
    \begin{enumerate}
        \item LOAD 8(R2) $\rightarrow$ R3
        \item STORE R4 $\rightarrow$ 4(R3)
    \end{enumerate}
    This will not work, because when the LOAD instruction is in the \textit{Write-back Stage}, and the STORE is in the \textit{Memory Stage}, the STORE instruction would have already calculated the address in the \textit{Execute Stage} and hence cannot recalculate the address with the updated value of R3 here. In this case, WM Bypassing will not work.
    \end{answered}
    
    \item Bypass Logic
    \begin{answered}
        \begin{equation*}
        \begin{split}
        ALUinA_{MUX} : &\ if\ (Execute.IR.src1 == Memory.IR.dest)\ \implies 0\\ &\ if\ (Execute.IR.src1 == Writeback.IR.dest)\ \implies 1\\ &\ else\ \implies 2
        \end{split}
        \end{equation*}
        \begin{equation*}
        \begin{split}
        ALUinB_{MUX} : &\ if\ (Execute.IR.src2 == Memory.IR.dest)\ \implies 0\\ &\ if\ (Execute.IR.src2 == Writeback.IR.dest)\ \implies 1\\ &\ else\ \implies 2
        \end{split}
        \end{equation*}        
    \end{answered}
    
    \item Bypass example:
    \begin{answered}
    \begin{center}
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
     \hline
     Cycle & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ 
     \hline
     ADD R2, R3 $\rightarrow$ R1 & F & D & X & M & \textbf{W}$\downarrow$ & - & - & - \\ 
     \textit{What goes here?} & - & F & D & X & \textbf{M } & W & - & - \\ 
     \hline
    \end{tabular}
    \end{center}
    First, for a \textbf{WM Bypass} to occur, we know that the instruction after the ADD instruction must have a dependence on register R1. 
    
    Second, this instruction cannot be writing to an address that depends on register R1. For \textit{example} the instruction [STORE R5 $\rightarrow$ 7(R1)] is not possible.
    
    With this in mind, a suitable instruction could be [\textbf{STORE R1 $\rightarrow$ 8(R5)}].
    \begin{center}
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
     \hline
     Cycle & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ 
     \hline
     ADD R2, R3 $\rightarrow$ R1 & F & D & X & M & \textbf{W}$\downarrow$ & - & - & - \\ 
     STORE R1 $\rightarrow$ 7(R5) & - & F & D & X & \textbf{M } & W & - & - \\ 
     \hline
    \end{tabular}
    \end{center}    
    \end{answered}
    
    \item Load-to-Use Dependency
    \begin{answered}
    Have we prevented all possible data hazards with the above 3 bypassing techniques? \textbf{No}. The load-to-use case is an example where bypassing just doesn't cut it and we have to use a \textbf{Stall}.
    
    For \textit{example:} consider the following instructions:
    \begin{enumerate}
        \item LOAD 8(R2) $\rightarrow$ R3
        \item ADD R2, R3 $\rightarrow$ R4
    \end{enumerate}
    When the LOAD instruction is in the \textit{Memory Stage} and the ADD instruction is in the \textit{Execute Stage}, the LOAD instruction cannot forward the updated value of R3 yet since it hasn't accessed the \textit{Data Memory} yet to perform the address calculation. 
    
    We need to therefore \textbf{identify} this situation early on and prevent the ADD instruction from advancing beyond the \textit{Decode Stage} by inserting a \textbf{stall/bubble} into the \textit{Execute Stage}.
    
    This condition can be identified as follows:
    \begin{equation*}
    \begin{split}
    Stall = &\ (Execute.IR.operation == LOAD)\ \&\& \\ &\ (\ (Decode.IR.src1 == Execute.IR.dest)\ || \\ &\ \ \ ((Decode.IR.src2 == Execute.IR.dest) \&\& \\ &\ \ \ \ (Decode.IR.op != STORE)) \\ &)
    \end{split}
    \end{equation*}     
    
    Once this condition is detected as above, a stall or bubble can be inserted. In the following cycle, a regular \textbf{WX Bypass} can be used.
    
    \textit{Performance:} Assuming that 50\% of loads are followed by dependent instructions (load-to-use), then this will require 1 stall cycle (an insertion of a no-op).
    
    The resulting performance will then be CPI = 1 + (1 $\times$ 0.20 $\times$ 0.50).
    
    We can also include additional optimization by re-arranging instructions as seen with \textit{software interlocks}, but this time we do it for performance and not for correctness. 
    \end{answered}

    \item Structural Hazards:
    \begin{answered}
        A structural hazard is when two instructions are trying to use a piece of the circuit at the exact same time. For \textit{example:} two instructions trying to use a write port at the same time. 
        
        This can be avoided under the following conditions:
        \begin{itemize}
            \item Each instruction uses every structure in the circuit exactly once
            \item Each instruction does this for at most one cycle
            \item All instructions travel through all stages of the circuit
        \end{itemize}
    \end{answered}
    
    \item Structural Hazards and Multiple-Cycle Operations
    \begin{answered}
    Another example of a \textbf{Structural Hazard} occurring is when we have operations that don't follow the assumption that all instructions are going to go through all the different stages of the pipeline. 
    
    \textbf{Multi-cycle Multiply:}
    
    For \textit{example:} If you have a multiply instruction, it is a high latency operation - \textit{Execute Stage} gets really large because of this. We can then put it through it's own separate pipeline. We can then run the original part of the pipeline faster, but multiply operations will take 4 clock cycles. It has it's own register and lives as it's own unit. We can also make it skip the \textit{Memory Stage} and go straight to the \textit{Write-back Stage}.
    
    Let's look at some of the implications of this approach:
    
    \textbf{Case 1:} \textit{What happens during a regular data dependence condition?}
    
    \begin{center}
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
     \hline
     Cycle & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\ 
     \hline
     MUL R3, R5 $\rightarrow$ R4 & F & D & P0 & P1 & P2 & P3 & \textbf{W}$\downarrow$ & - & -\\ 
     ADDI R4, 1 $\rightarrow$ R6 & - & F & D & $d^{*}$ & $d^{*}$ & $d^{*}$ & \textbf{X } & M & W \\ 
     \hline
    \end{tabular}
    \end{center} 
    
    A sequence of instructions such as the one above will result in 3 stalls!
    
    The stall logic to identify this condition is:
    \begin{equation*}
    \begin{split}
    Stall & = (PreviousStallLogic)|| \\ & (Decode.IR.src1 == PO.IR.dest)||  (Decode.IR.src2 == PO.IR.dest)|| \\ & (Decode.IR.src1 == P1.IR.dest)|| 
    (Decode.IR.src2 == P1.IR.dest)|| \\ & (Decode.IR.src1 == P2.IR.dest)|| 
    (Decode.IR.src2 == P2.IR.dest)
    \end{split}
    \end{equation*}    
    
    \textbf{Case 2:} \textit{What if two instructions are trying to write to the register file in the same cycle?}
    \begin{equation*}
    \begin{split}
    Stall & = (PreviousStallLogic)\ || \\ &\ \ \ \ (Decode.IR.dest == "valid")\ \&\& \\  &\ \ \ \ (Decode.IR.operation \neq MULT)\ \&\& \\ &\ \ \ \  (P1.IR.dest == "valid")
    \end{split}
    \end{equation*}  
    
    Here, being \textit{valid} indicates that it is indeed writing to a register since not all instructions must write to a register. You could also avoid this problem by introducing an additional write port, but that requires additional hardware and comes with it's own additional overhead and latency.
    
    \textbf{Case 3:} \textit{Since multiplies take 4 cycles to complete, what if the cycle that comes after it finishes before it?}
    \begin{center}
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
     \hline
     Cycle & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\ 
     \hline
     MUL R3, R5 $\rightarrow$ R4 & F & D & P0 & P1 & P2 & P3 & \textbf{W} & - & -\\ 
     ADDI R1, 1 $\rightarrow$ R4 & - & F & D & X & M & \textbf{W} & - & - & - \\ 
     ... & - & - & - & - & - & - & - & - & - \\
     ... & - & - & - & - & - & - & - & - & - \\
     ADD R4, R6 $\rightarrow$ R8 & - & - & - & - & F & D & X & M & W \\ 
     \hline
    \end{tabular}
    \end{center} 
    
    The value of $R4$ that the ADD instruction will recieve is most likely to be incorrect because of the mis-ordering.
    
    We can identify this condition as follows:
    \begin{equation*}
    \begin{split}
    Stall & = (PreviousStallLogic)\ || \\ &\ \ \ \ ((Decode.IR.dest == P0.IR.dest)\ || \\  &\ \ \ \ \ (Decode.IR.dest == P1.IR.dest))\ 
    \end{split}
    \end{equation*} 
    
    \textbf{Summary:} 
    
    Multi-cycle operations complicate pipeline logic, but there are definite performance gains to be had. It is an \textit{area-efficient} way to add throughput. However, in some cases such as in int/FP divide, it's difficult to do and may just not be worth it, in which case we should just accept the structural hazard and stall for a few cycles. 
    \end{answered}
\end{QandA}

\newpage 

\section{Textbook Notes}
\begin{QandA}
   \item Pipelining Paradox:
   \begin{answered}
    The time from placing a single instruction into the pipeline till it's finished is \textbf{not} faster for pipelining. The reason pipelining \textbf{is faster} is because when many instructions go through the pipeline, instructions in different stages of the pipeline can work in parallel and more instructions can be finished in a given unit of time. \textit{Throughput} is what we improve, not \textit{latency}. But \textit{throughput} is important because computers execute billions of instructions!
   \end{answered}
   
   \item Performance:
   \begin{answered}
   Under ideal conditions and with a large number of instructions, the speed-up from pipelining is approximately equal to the number of pipeline stages. For \textit{example} a five-stage pipeline is nearly five times faster. 
   
   There are however \textbf{start-up} and \textbf{wind-down} effects that lower performance when the number of tasks is not large compared to the number of stages. For \textit{example} a 5 stage pipeline with 2 instructions isn't a huge improvement in performance. 
   
   Additionally, dependence problems and stall cycles further reduce performance indicating that the speed-up is usually less than the number of pipeline stages.
   \begin{equation*}
       \text{speedup}_{pipeline} \leq \text{number of pipeline stage}
   \end{equation*}
      \begin{equation*}
       \text{speedup}_{pipeline} = \text{\#stages} - (\text{start-up \& wind-down} + \text{dependencies})
   \end{equation*}
   \end{answered}
   
   \item Conditions that make pipelining easier?
   \begin{answered}
   \begin{enumerate}
       \item LC4 instructions are all the same length
       \item LC4 instructions have only a few unique formats
       \item LC4 instructions have only LOAD and STORE memory instructions
       \item LC4 instructions have operands aligned in memory (only one access required)
   \end{enumerate}
   \end{answered}
   
   \item Does bypassing solve all dependence problems?
   \begin{answered}
   No, sometimes we just have to take the \textit{stall} and move on. An \textit{example} is the \textbf{Load-to-Use} condition. 
   \end{answered}
   
   \item What are the consequences of allowing JUMP, BRANCH and ALU instructions to take fewer stages than the five required?
   \begin{answered}
   This would create more problems than it's worth. Additionally, ALU instructions require to go through till the fifth stage since they write back to the destination register. Essentially, we should be looking to optimize our pipeline to require shorter cycles, even if that means splitting the pipeline into more stages.
   \end{answered}
   
   \item What happens when a register is read and written in the same clock cycle?
   \begin{answered}
   One way to resolve this problem is to design the register file to enforce \textit{Read-after-Write (RAW)} which means that the write is performed in the first half of the clock cycle and read is in the second half of the clock cycle. 
   
   Alternatively, we could also have \textit{Write-after-Read (WAR)} and derive our correctness guarantees accordingly. 
   \end{answered}
   
   \item Bypass all instructions all the time (!?)
   \begin{answered}
   Some instructions do not write registers, so attempting to add bypassing for all instructions isn't accurate. However, an easily fix for this problem is to check if the \textit{RegisterWrite} signal is \textbf{active} for that particular instruction and decide to bypass accordingly. 
   \end{answered}
   
   \item MX Bypass
   \begin{answered}
   If the destination register of the instruction in the X$\rightarrow$\textbf{M} stage is the same register as any one of the source registers of the instruction in the D$\rightarrow$\textbf{X} stage, \textbf{and} if a \textit{RegisterWrite} signal is enabled, then a \textbf{MX Bypass} is required.  
   
    \begin{center}
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
     \hline
     Cycle & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\ 
     \hline
     SUB R1, R3 $\rightarrow$ \textit{R2} & F & D & X & \textbf{M}$\downarrow$ & W & - & - & - & -\\ 
     AND \textit{R2}, R5 $\rightarrow$ R12 & - & F & D & \textbf{X } & M & W & - & - & - \\ 
     \hline
    \end{tabular}
    \end{center}    
   \end{answered}
   
   \item WX Bypass
   \begin{answered}
   If the destination register of the instruction in the M$\rightarrow$\textbf{W} stage is the same register as any one of the source registers of the instruction in the D$\rightarrow$\textbf{X} stage, \textbf{and} if a \textit{RegisterWrite} signal is enabled, then a \textbf{WX Bypass} is required.  
    \begin{center}
    \begin{tabular}{ |c|c|c|c|c|c|c|c|c|c| } 
     \hline
     Cycle & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\ 
     \hline
     SUB R1, R3 $\rightarrow$ \textit{R2} & F & D & X & \textbf{M}$\downarrow$ & \textbf{W}$\downarrow$$\downarrow$ & - & - & - & -\\ 
     AND \textit{R2}, R5 $\rightarrow$ R12 & - & F & D & \textbf{X  } & M & W & - & - & - \\ 
     OR R6, \textit{R2} $\rightarrow$ R13 & - & - & F & D & \textbf{X  } & M & W & - & - \\ 
     \hline
    \end{tabular}
    \end{center}   
   \end{answered}
   
   \item WM Bypass
   \begin{answered}
   This is a more specific bypass mechanism when you have a LOAD instruction immediately followed by a STORE instruction. This is usually seen in memory-to-memory copies. 
   \end{answered}
   
   \item Load-to-Use 
   \begin{answered}
   There is a case where bypassing cannot save the day. This occurs when a load instruction is followed by an instruction that tries to read from the same register that the load instruction writes to.
   
   To detect this condition: check to see if the instruction is a load instruction and check if the destination register field of the load in the \textit{Execute Stage} matches either of the source registers in the \textit{Decode Stage}. Once this condition is detected, a \textbf{bubble} is inserted to stall the pipeline for one cycle. 
   \end{answered}
   
   
\end{QandA}

\end{document}

