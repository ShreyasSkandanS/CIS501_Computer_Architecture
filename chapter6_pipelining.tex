\documentclass[12pt]{article}
%\documentclass[10pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{authblk}
\usepackage{indentfirst}
\usepackage{enumitem}
\newenvironment{QandA}{\begin{enumerate}[label=\bfseries\alph*.]\bfseries}
                      {\end{enumerate}}
\newenvironment{answered}{\par\quad\normalfont}{}

\title{CIS501: Pipelining}
\author[1]{Shreyas S. Shivakumar}

\begin{document}

\maketitle

\section{Lecture Notes}

\begin{QandA}
   \item What are we trying to improve by introducing this optimization (pipelining)?
        \begin{answered}
        Throughput. In the \textit{laundry example}, pipelining will only enhance performance if you have more than one load of clothes to clean.
        \end{answered}
        
    \item If you have 3 stages - washer, dryer and "folding robot"? How long does one load of laundry take? How long does two loads of laundry take? How long would 100 loads take?
        \begin{answered}
            $1$ load of laundry will take $3$ units of time. $2$ loads of laundry will take $4$ units of time. $100$ loads of laundry will take $104$ units of time. 
            
            To understand why $100$ loads of laundry will take $104$ units of time, understand why $3$ loads of laundry will take $5$ loads of time. \textit{Intuition:} The case(s) when the number of loads of laundry is $\geq$ the number of pipeline stages.
        \end{answered}
        
    \item What is the processor performance equation?
    \begin{answered}
    \begin{equation*}
        Execution\ Time = \frac{Seconds}{Program} = \frac{I \times S \times C}{P \times C \times I}
    \end{equation*}
    \begin{equation*}
        Execution\ Time = \frac{Instructions}{Program} \times \frac{Seconds}{Cycle} \times \frac{Cycles}{Instruction}
    \end{equation*}
    
    For \textit{performance optimization}, we want to ideally minimize all three of the above terms, but sometimes they will pull against each other.
    
    Here $\frac{Instruction}{Program}$ refers to the \textbf{Dynamic Instruction Count}. It is the instructions that the processor will actually run during the course of a program and not just the static instructions (eg: loops).
    \end{answered}
    
    \item Briefly discuss the performance of a Single-Cycle Datapath and mention it's limitations?
    \begin{answered}
    Everything is done within a single clock cycle. This means that the clock cycle is chosen to be the worst-case delay through the circuit - \textit{critical path}. Therefore, performance is limited by the slowest instruction always.
    \end{answered}
        
    \item What are the different stages in the pipelined LC4 datapath?
    \begin{answered}
    There are five different stages in the LC4 datapath, called the \textbf{pipeline depth}, with one instruction in each stage in each cycle:
    \begin{itemize}
        \item Fetch (\textbf{F}) : 
        \item Decode (\textbf{D})
        \item Execute (\textbf{X})
        \item Memory (\textbf{M})
        \item Writeback (\textbf{W})
    \end{itemize}
    \end{answered}
    
    \item How is the clock period selected for this pipelined datapath?
    \begin{answered}
    \begin{equation*}
        Clock\ Period\ =\ MAX(T_{F}, T_{D}, T_{X}, T_{M}, T_{W})
    \end{equation*}    
    \end{answered}
    
    \item What is the CPI of the pipelined architecutre?
    \begin{answered}
    The base CPI is $1$ because instructions enter and leave every cycle, but the actual CPI $\geq$ 1 since the pipeline might \textit{stall}. The individual instruction \textbf{latency} will actually \textbf{increase} due to the overhead of the pipelining process, but the benefits of additional throughput make it worth sacrificing latency for.
    \end{answered}
    
    \item How are the different stages arranged?
    \begin{answered}
    The different stages are delimited by the \textit{pipeline registers}, which are named by the stages they begin at.
    \begin{itemize}
        \item PC Register \textbf{(PC)} \textit{(Fetch)}
        \item Decode Register \textbf{(D)}
        \item Execute Register \textbf{(X)}
        \item Memory Register \textbf{(M)}
        \item Writeback Register \textbf{(W)}
    \end{itemize}
    \end{answered}
    
    \item Pipeline Example:
    \begin{answered}
    Let's look at the following example of three instructions:
    \begin{enumerate}
        \item ADD R1, R2, R3 : \textit{where R3 is the destination register}
        \item LOAD 8(R5), R4 : \textit{where R4 is the destination register}
        \item STORE R6, 4(R7) : \textit{where R7+4 is the destination memory address}
    \end{enumerate}
    
    \textbf{Cycle 1:} 
    
    In the first cycle, we do not yet know that the instruction entering the pipeline is an ADD instruction. This instruction enters the \textit{Fetch Stage} and we identify the address of the next instruction (in \textit{instruction memory}) using the \textbf{Program Counter (PC)}. We then go get those bits from the instruction memory.
    
    \textbf{Cycle 2:} 
    
    In the second cycle, the ADD instruction enters the \textit{Decode Stage} and proceeds to read from the \textit{Register File} to get the input values (R1 and R2). The decoding process parses the input instruction to establish all the control signals required for this instruction in the proceeding stages.
    
    \quad In this same cycle, the next instruction (LOAD) is in the \textit{Fetch} stage, doing what the ADD instruction did in the previous cycle.
    
    \textbf{Cycle 3:} 
    
    In the third cycle, the ADD instruction enters the \textit{Execute Stage} and proceeds to use the \textbf{ALU} to execute an ADD operation on the two operands. 
    
    \quad In this same cycle, the LOAD instruction enters the \textit{Decode Stage} and the next instruction (STORE) enters the \textit{Fetch Stage}.
    
    \textbf{Cycle 4:}
    
    In the fourth cycle, the ADD instruction goes through the \textit{Memory Stage} even though there's nothing for it to do there. It needs to write the resulting value to register R3, which it will do in the next step. Can be thought of as a \textbf{NO-OP} stage for the ADD instruction.
    
    \quad The other two instructions proceed down the pipeline to \textit{Execute Stage} and \textit{Decode Stage} respectively.
    
    \textbf{Cycle 5:}
    
    In the fifth cycle, the ADD instruction enters the \textit{Writeback Stage}, and then performs a write-back of the ALU result to the \textit{Register File}, selecting the correct destination register.
    
    \quad The other two instructions proceed down the pipeline to \textit{Memory Stage} and \textit{Execute Stage} respectively. 
    
    \textbf{Cycle 6 \& Cycle 7:}
    
    LOAD and STORE pass through the last two stages of the pipeline over the these two cycles.
    
    \end{answered}
    
    \item Pipeline Performance:
    \begin{answered}
    In a single-cycle design, if the clock period is \textbf{50ns} and CPI = 1, then your effective performance is \textbf{50ns/insn}.
    
    In the 5-stage pipeline above, you could naively say the performance is now $\frac{50ns}{5}$ = \textbf{10ns}, but this is not the case for the following reasons:
    \begin{itemize}
        \item Not all stages are uniform in the amount of time they will take.
        \item Pipeline registers add delay to the system
        \item There are more datapaths in pipelined sytems (bypasses)
    \end{itemize}
    
    Therefore longer (deeper) pipelines show diminishing clock frequency gains. However, in our example, we can assume that our clock period = \textbf{10ns} + overheads = \textbf{12ns}. And let's say that our CPI = 1 + pipeline penalty = \textbf{1.5}. Our effective performance will then be \textbf{12ns $\times$ 1.5 = 18ns/insn}.
    \end{answered}
    
    \item Dependencies and Hazards
    \begin{answered}
    A key challenge in pipelining is the identification and handling of dependencies across multiple different instructions.
    
    \textbf{Dependence:} It is a relationship between any two instructions.
    
    \begin{itemize}
        \item Data Dependence: If two instructions use the same storage locations (i.e either the same memory address or register)
        \item Control Dependence: One instruction affects whether another instruction executes at all (i.e such as in a branch instruction)
    \end{itemize}
    
    \textbf{Hazard:} A hazard is a result of the existence of a dependence leading to the possibility of wrong instruction order. The effects of a hazard should not be externally visible. \textbf{Stalls} are used to maintain this order by holding a younger instruction in the same stage and propagating an empty instruction (\textbf{no-op}) instead. Hazards are undesirable because the stalls that are introduce result in reduction in performance.
    \end{answered}
    
    \item Data Hazards
    \begin{answered}
    Ignoring \textit{control hazards} for now, let us look at Data Hazards, i.e when two instructions use the same storage locations (memory or register). 
    
    \textit{Example 1:}
    \begin{enumerate}
        \item ADD R1, R2 $\rightarrow$ \textbf{R3}
        \item ADD \textbf{R3}, R5 $\rightarrow$ R6
    \end{enumerate}
    
    \textit{Example 2:}
    \begin{enumerate}
        \item ADD R1, R2 $\rightarrow$ \textbf{R3}
        \item LOAD 8(\textbf{R3}) $\rightarrow$ R4
        \item ADDI 1, \textbf{R3} $\rightarrow$ R6
        \item STORE 8, R7 $\rightarrow$ \textbf{R3}
    \end{enumerate}
    
    At \textbf{Cycle 4} of this example, ADD is writing it's result to R3. LOAD has already read R3 two cycles ago, which means that the value that it currently has is outdated! ADDI also read R3 one cycle ago which means that it's value is outdated too. STORE is currently in the process of reading R3 but it is unclear whether it has the updated value of R3 or not depending on how the \textit{Register File} was implemented (i.e Read-after-Write or Write-after-Read).
    
    \end{answered}

    \item What are some initial ways we can try fix this problem?
    \begin{answered}
    We can fix this problem in two ways:
    \begin{enumerate}
        \item \textbf{Software Interlocks:} 
        
        The compiler will identify and put two independent instructions between the two instructions that have a dependence. If no independent instructions exist to re-order between them, a \textit{no-op} is placed.
        
        \begin{itemize}
            \item For software interlocks, the CPI is still technically 1 but there are now more instructions. For \textit{example}: 20\% of instructions require the insertion of 1 \textbf{no-op}. 5\% of instructions require the insertion of 2 \textbf{no-ops}.
            \item The total number of instructions then goes up to $1 + 0.20 \times 1 + 0.05 \times 2$ = 1.3 resulting in a 30\% slowdown.
            \item Another problem with software interlocks is the dependence on specific architecture design. These optimizations are pipeline dependent and make it difficult to ensure backward compatibility (e.g if you were to move towards a 7-stage pipeline, this would not work anymore).
        \end{itemize}
        
        \item \textbf{Hardware Interlocks:}
        
        Here, the processor detects the data hazards and fixes them. We are not concerned anymore with compatibility issues.
        
        We can \textit{detect data hazards} by comparing the input register names of the instruction in the \textbf{Decode Stage} with output register names of instructions in the \textbf{Execute Stage} and the \textbf{Memory Stage}.
        
        \begin{equation*}
        \begin{split}
        stall = &\ (Decode.IR.src1 == Execute.IR.dest)\ ||\\ &\ (Decode.IR.src2 == Execute.IR.dest)\ ||\\ &\ (Decode.IR.src1 == Memory.IR.dest)\ ||\\ &\ (Decode.IR.src2 == Memory.IR.dest)
        \end{split}
        \end{equation*}
        
        If any of these conditions hold \textit{True}, then insert a \textbf{no-op} into the pipeline at the next stage. This maneuver is called a \textbf{stall} or a \textbf{bubble}. 
        
        For hardware interlocks, the CPI can be calculated as above. Here 20\% of instructions require 1 cycle stall and 5\% of instructions require a 2\% cycle stall. The effective CPI is still the same as with \textit{software interlocks}, except here the instructions stay at 1 and the CPI increases by 30\%.

    \end{enumerate}
    \end{answered}
    
    \item Bypassing
    \begin{answered}
    For \textit{example}: say we had an instruction [ADD R1, R2 $\rightarrow$ R3] in the \textit{Memory Stage} and an instruction [LOAD 8(R3) $\rightarrow$ R4] in the \textit{Execute Stage}. Since the LOAD instruction is already in the Execute Stage, this tells us that the current value of R3 that the LOAD instruction has is outdated, since the ADD instruction is just about to update it's value. However, at this very moment, nothing catastrophic has happened yet, even though we know it will in the next cycle if left unchecked. 
    
    \textbf{1. MX Bypass:}
    
    \textit{What if we were to pass the updated value of R3 from the Memory Stage back to the Execute Stage, so that the LOAD instruction can now possess the latest value for R3?}
    
    This is called \textbf{Bypassing} or \textbf{Forwarding} and this specific case of forwarding information from the \textit{Memory Stage} to the \textit{Execute Stage} is called \textbf{MX Bypassing}.
    
    There is indeed an additional hardware overhead of performing a bypass operation since we need extra wires and a multiplexer. But these overheads are negligible in terms of the performance gained. Instead of filling that cycle with a \textit{no-op} we were able to make progress at the cost of minimum hardware latency. 
    
    \textbf{2. WX Bypass:}
    
    Similarly, if such a dependence exists across the \textit{Write-back Stage} and the \textit{Execute Stage}, we can perform the same operation and forward this value to the \textit{Execute Stage} with an additional wire and Mux. 
    
    \begin{itemize}
        \item \textit{Which ALU input in the Execute Stage can you bypass to?} You can add similar wires and multiplexers and make bypass operations available to both inputs of the ALU. Both \textbf{ALUinA} and \textbf{ALUinB} bypassing is possible. 
    \end{itemize}
    
    \textbf{3. WM Bypass:}
    
    Another bypassing technique, although helpful in limited circumstances, is WM Bypass. For \textit{example} if you have instructions [LOAD 8(R2) $\rightarrow$ R3] and [STORE R3 $\rightarrow$ 4(R4)]. When the LOAD instruction is in the \textit{Write-back Stage} it writes the updated value of register R3 back to the Register File. However, the STORE instruction needs to write the value from register R3 to memory address $4(R4)$. Currently, the STORE instruction contains an outdated value of R3 and hence a bypass from the \textit{Write-back Stage} to the \textit{Memory Stage} makes sense.
    
    However, let us look at an \textbf{important contradicting example}:
    \begin{enumerate}
        \item LOAD 8(R2) $\rightarrow$ R3
        \item STORE R4 $\rightarrow$ 4(R3)
    \end{enumerate}
    This will not work, because when the LOAD instruction is in the \textit{Write-back Stage}, and the STORE is in the \textit{Memory Stage}, the STORE instruction would have already calculated the address in the \textit{Execute Stage} and hence cannot recalculate the address with the updated value of R3 here. In this case, WM Bypassing will not work.
    \end{answered}
    
    \item Bypass Logic
    \begin{answered}
        \begin{equation*}
        \begin{split}
        ALUinA_{MUX} : &\ if\ (Execute.IR.src1 == Memory.IR.dest)\ \implies 0\\ &\ if\ (Execute.IR.src1 == Writeback.IR.dest)\ \implies 1\\ &\ else\ \implies 2
        \end{split}
        \end{equation*}
        \begin{equation*}
        \begin{split}
        ALUinB_{MUX} : &\ if\ (Execute.IR.src2 == Memory.IR.dest)\ \implies 0\\ &\ if\ (Execute.IR.src2 == Writeback.IR.dest)\ \implies 1\\ &\ else\ \implies 2
        \end{split}
        \end{equation*}        
    \end{answered}


\end{QandA}

\end{document}

